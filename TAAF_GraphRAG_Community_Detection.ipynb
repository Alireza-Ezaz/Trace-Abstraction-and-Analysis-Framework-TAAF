{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Introduction**\n",
        "This notebook integrates the TAAF approach for knowledge-graph-based question answering with an optional local→global community-based method from the paper. We have already performed local community detection in a separate script."
      ],
      "metadata": {
        "id": "NzFQTqXzgH9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Upload the Processed Graph File**\n",
        "Here, we upload the knowledge_graph_with_communities.json generated by the local script."
      ],
      "metadata": {
        "id": "q7vCdwqB8NXA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "collapsed": true,
        "id": "N3cvqmzT7zdr",
        "outputId": "f1883cee-4b34-49cf-bd1c-c6b5f853a2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload 'knowledge_graph_with_communities.json' now.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f12030ad-0d2c-4950-917b-dcca9ce84b2a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f12030ad-0d2c-4950-917b-dcca9ce84b2a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ec189b96ec0a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please upload 'knowledge_graph_with_communities.json' now.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# user selects the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'knowledge_graph_with_communities.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Step 2: Upload knowledge_graph_with_communities.json\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "print(\"Please upload 'knowledge_graph_with_communities.json' now.\")\n",
        "uploaded = files.upload()  # user selects the file\n",
        "\n",
        "with open('knowledge_graph_with_communities.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "nodes = data['nodes']\n",
        "links = data['links']\n",
        "\n",
        "print(f\"Loaded {len(nodes)} nodes and {len(links)} links from the new file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 (Markdown): Install and Import Necessary Packages**\n",
        "We install neo4j to connect to the database, plus openai and tiktoken for GPT-based query generation and summarization. We also import standard libraries (time, json, etc.)."
      ],
      "metadata": {
        "id": "OCPm7ceT8Sg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Install and Import Packages\n",
        "!pip install neo4j openai tiktoken numpy\n",
        "\n",
        "import openai\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "from neo4j import GraphDatabase, Session\n",
        "from neo4j.exceptions import ServiceUnavailable\n",
        "\n",
        "print(\"Packages installed and imported.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q_5ixfqK8XtQ",
        "outputId": "95dc6c8c-ddbb-4330-8b88-bf23ad4d6364"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neo4j\n",
            "  Downloading neo4j-5.27.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.6)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from neo4j) (2024.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Downloading neo4j-5.27.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.7/301.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neo4j, tiktoken\n",
            "Successfully installed neo4j-5.27.0 tiktoken-0.8.0\n",
            "Packages installed and imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Connect to Neo4j**\n",
        "We set up our Neo4j driver with credentials. If using Colab’s secret store (userdata), we retrieve them; else we set them manually."
      ],
      "metadata": {
        "id": "RVmby3G-8_vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Connect to Neo4j\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    openai.api_key = userdata.get('OPENAI_API_KEY')  # or \"YOUR_OPENAI_API_KEY\"\n",
        "    uri = userdata.get('NEO4J_URI')                  # e.g. 'neo4j+s://XYZ.databases.neo4j.io'\n",
        "    password = userdata.get('NEO4J_PASSWORD')        # or \"YOUR_NEO4J_PASSWORD\"\n",
        "except:\n",
        "    openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "    uri = \"neo4j+s://YOUR-NEO4J-ENDPOINT\"\n",
        "    password = \"YOUR-NEO4J-PASSWORD\"\n",
        "\n",
        "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", password))\n",
        "print(\"Connected to Neo4j successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zYlrf9xP9C3x",
        "outputId": "340250c9-b3c6-445d-b318-9bbac83d3f5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Neo4j successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5 : Create Nodes and Relationships in Neo4j**\n",
        "We define helper functions to load our newly community-enriched nodes and relationships into Neo4j. Each node has an id property, a label, an optional communityId, and the original entity label in Neo4j."
      ],
      "metadata": {
        "id": "fFABaPpFExKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create Nodes and Relationships in Neo4j\n",
        "\n",
        "def create_nodes(tx: Session, nodes_data):\n",
        "    \"\"\"\n",
        "    MERGE each node by its 'id'. We store 'label' and 'communityId' if present.\n",
        "    'entity' is used as the node label in Neo4j (e.g., CPU, Thread).\n",
        "    \"\"\"\n",
        "    for node in nodes_data:\n",
        "        entity_label = node.get('entity', 'Generic')\n",
        "        query_str = []\n",
        "        query_str.append(f\"MERGE (n:{entity_label} {{id: $id}})\")\n",
        "        query_str.append(\"SET n.label = $n_label\")\n",
        "\n",
        "        if \"communityId\" in node:\n",
        "            query_str.append(\"SET n.communityId = $communityId\")\n",
        "\n",
        "        final_query = \"\\n\".join(query_str)\n",
        "\n",
        "        tx.run(\n",
        "            final_query,\n",
        "            id=node['id'],\n",
        "            n_label=node.get('label', ''),\n",
        "            communityId=node.get('communityId', None)\n",
        "        )\n",
        "\n",
        "def create_relationships(tx: Session, links_data):\n",
        "    \"\"\"\n",
        "    MERGE each relationship. 'relationship' is used as the rel type (spaces replaced).\n",
        "    'weight' property is stored if present, else default to 1.\n",
        "    \"\"\"\n",
        "    for link in links_data:\n",
        "        source = link['source']\n",
        "        target = link['target']\n",
        "        rel_type = link['relationship'].replace(' ', '_')\n",
        "        weight = link.get('weight', 1)\n",
        "\n",
        "        q = f\"\"\"\n",
        "        MATCH (a {{id: $source}}), (b {{id: $target}})\n",
        "        MERGE (a)-[r:{rel_type} {{weight: $weight}}]->(b)\n",
        "        \"\"\"\n",
        "        tx.run(q, source=source, target=target, weight=weight)\n",
        "\n",
        "def load_data_into_neo4j(nodes, links):\n",
        "    with driver.session() as session:\n",
        "        try:\n",
        "            session.write_transaction(create_nodes, nodes)\n",
        "            session.write_transaction(create_relationships, links)\n",
        "            print(\"Data loaded into Neo4j successfully.\")\n",
        "        except ServiceUnavailable as e:\n",
        "            print(\"Error loading data into Neo4j:\", e)\n",
        "\n",
        "# We'll run the load in the next step."
      ],
      "metadata": {
        "collapsed": true,
        "id": "qtFp8UpzE0xp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Execute the Loading**\n",
        "We now call the loading function, creating all nodes and relationships in your Neo4j DB."
      ],
      "metadata": {
        "id": "BVw3swtTE-4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Execute the loading\n",
        "load_data_into_neo4j(nodes, links)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rchfrHMLFCLC",
        "outputId": "d0f1d6b5-a561-4698-d69b-8eec5534a43f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-9dc63f9e59d2>:46: DeprecationWarning: write_transaction has been renamed to execute_write\n",
            "  session.write_transaction(create_nodes, nodes)\n",
            "<ipython-input-17-9dc63f9e59d2>:47: DeprecationWarning: write_transaction has been renamed to execute_write\n",
            "  session.write_transaction(create_relationships, links)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded into Neo4j successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Retrieve Node/Relationship Schema (Optional)**\"We can introspect the database to see what node labels and relationship types exist, along with their properties. This can feed into GPT-based query generation as context."
      ],
      "metadata": {
        "id": "-qygiR-xFLAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Retrieve Node/Relationship Schema\n",
        "\n",
        "def rec_to_type(val):\n",
        "    if isinstance(val, int):\n",
        "        return {\"type\": \"Integer\", \"example_value\": str(val)}\n",
        "    elif isinstance(val, float):\n",
        "        return {\"type\": \"Float\", \"example_value\": str(val)}\n",
        "    elif isinstance(val, bool):\n",
        "        return {\"type\": \"Boolean\", \"example_value\": str(val)}\n",
        "    elif isinstance(val, list):\n",
        "        return {\"type\": \"List\", \"example_value\": str(val)}\n",
        "    else:\n",
        "        return {\"type\": \"String\", \"example_value\": str(val)}\n",
        "\n",
        "def get_node_labels_and_properties():\n",
        "    with driver.session() as session:\n",
        "        labels_res = session.run(\"CALL db.labels()\")\n",
        "        labels = [r['label'] for r in labels_res]\n",
        "        label_props = {}\n",
        "        for lab in labels:\n",
        "            q = f\"\"\"\n",
        "            MATCH (n:`{lab}`)\n",
        "            UNWIND keys(n) AS k\n",
        "            WITH k, head(collect(n[k])) AS val\n",
        "            RETURN DISTINCT k, val\n",
        "            \"\"\"\n",
        "            props_res = session.run(q)\n",
        "            props_dict = {}\n",
        "            for rec in props_res:\n",
        "                k = rec['k']\n",
        "                v = rec['val']\n",
        "                props_dict[k] = rec_to_type(v)\n",
        "            label_props[lab] = props_dict\n",
        "    return label_props\n",
        "\n",
        "def get_relationship_types_and_properties():\n",
        "    with driver.session() as session:\n",
        "        rel_types_res = session.run(\"CALL db.relationshipTypes()\")\n",
        "        rel_types = [r['relationshipType'] for r in rel_types_res]\n",
        "        info = {}\n",
        "        for rt in rel_types:\n",
        "            # Gather props\n",
        "            pquery = f\"\"\"\n",
        "            MATCH ()-[r:`{rt}`]->()\n",
        "            UNWIND keys(r) AS k\n",
        "            WITH k, head(collect(r[k])) AS val\n",
        "            RETURN DISTINCT k, val\n",
        "            \"\"\"\n",
        "            p_res = session.run(pquery)\n",
        "            props = {}\n",
        "            for rec in p_res:\n",
        "                k = rec['k']\n",
        "                v = rec['val']\n",
        "                props[k] = rec_to_type(v)\n",
        "\n",
        "            # Gather start/end labels\n",
        "            cquery = f\"\"\"\n",
        "            MATCH (s)-[r:`{rt}`]->(e)\n",
        "            RETURN DISTINCT labels(s) AS startLabels, labels(e) AS endLabels\n",
        "            \"\"\"\n",
        "            c_res = session.run(cquery)\n",
        "            pairs = set()\n",
        "            for row in c_res:\n",
        "                for sl in row['startLabels']:\n",
        "                    for el in row['endLabels']:\n",
        "                        pairs.add((sl, el))\n",
        "            startLabs = list(set([p[0] for p in pairs]))\n",
        "            endLabs = list(set([p[1] for p in pairs]))\n",
        "\n",
        "            info[rt] = {\"properties\": props, \"startLabels\": startLabs, \"endLabels\": endLabs}\n",
        "\n",
        "    return info\n",
        "\n",
        "node_schema = get_node_labels_and_properties()\n",
        "rel_schema = get_relationship_types_and_properties()\n",
        "\n",
        "print(\"Node Schema:\")\n",
        "print(json.dumps(node_schema, indent=2))\n",
        "\n",
        "print(\"\\nRelationship Schema:\")\n",
        "print(json.dumps(rel_schema, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mS79mhRbFQY7",
        "outputId": "d49ff6bc-10e0-4ea6-dc0f-1fd565dde52e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Schema:\n",
            "{\n",
            "  \"CPU\": {\n",
            "    \"id\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"CPU_3\"\n",
            "    },\n",
            "    \"label\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"CPU 3\"\n",
            "    },\n",
            "    \"communityId\": {\n",
            "      \"type\": \"Integer\",\n",
            "      \"example_value\": \"0\"\n",
            "    }\n",
            "  },\n",
            "  \"Thread\": {\n",
            "    \"id\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"T_0\"\n",
            "    },\n",
            "    \"label\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"swapper/3 (T_0)\"\n",
            "    },\n",
            "    \"communityId\": {\n",
            "      \"type\": \"Integer\",\n",
            "      \"example_value\": \"0\"\n",
            "    }\n",
            "  },\n",
            "  \"File\": {\n",
            "    \"id\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"File_140637496150960\"\n",
            "    },\n",
            "    \"label\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"Events FD=140637496150960\"\n",
            "    },\n",
            "    \"communityId\": {\n",
            "      \"type\": \"Integer\",\n",
            "      \"example_value\": \"1\"\n",
            "    }\n",
            "  },\n",
            "  \"Process\": {\n",
            "    \"id\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"P_7\"\n",
            "    },\n",
            "    \"label\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"Process 7.0\"\n",
            "    },\n",
            "    \"communityId\": {\n",
            "      \"type\": \"Integer\",\n",
            "      \"example_value\": \"0\"\n",
            "    }\n",
            "  },\n",
            "  \"Network\": {\n",
            "    \"id\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"Socket_FD_46\"\n",
            "    },\n",
            "    \"label\": {\n",
            "      \"type\": \"String\",\n",
            "      \"example_value\": \"Socket FD 46\"\n",
            "    },\n",
            "    \"communityId\": {\n",
            "      \"type\": \"Integer\",\n",
            "      \"example_value\": \"0\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "Relationship Schema:\n",
            "{\n",
            "  \"switched_in\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"44\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"CPU\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"Thread\"\n",
            "    ]\n",
            "  },\n",
            "  \"switched_out\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"44\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"CPU\"\n",
            "    ]\n",
            "  },\n",
            "  \"scheduled_to_wake_on\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"23\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"CPU\"\n",
            "    ]\n",
            "  },\n",
            "  \"wake_up\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"19\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"CPU\"\n",
            "    ]\n",
            "  },\n",
            "  \"epoll_wait_exit\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"1\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"File\"\n",
            "    ]\n",
            "  },\n",
            "  \"ioctl\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"393\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"File\"\n",
            "    ]\n",
            "  },\n",
            "  \"splice_in\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"98\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"File\"\n",
            "    ]\n",
            "  },\n",
            "  \"ioctl_exit\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"393\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"Thread\"\n",
            "    ]\n",
            "  },\n",
            "  \"splice_exit\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"196\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"Thread\"\n",
            "    ]\n",
            "  },\n",
            "  \"sync_file_range_exit\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"196\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"Thread\"\n",
            "    ]\n",
            "  },\n",
            "  \"splice_out\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"98\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"File\"\n",
            "    ]\n",
            "  },\n",
            "  \"sync_file\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"196\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"File\"\n",
            "    ]\n",
            "  },\n",
            "  \"runtime_stat\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"4\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"CPU\"\n",
            "    ]\n",
            "  },\n",
            "  \"contains\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"1\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Process\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"Thread\"\n",
            "    ]\n",
            "  },\n",
            "  \"sendmsg_exit\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"1\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"Thread\"\n",
            "    ]\n",
            "  },\n",
            "  \"close_exit\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"1\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"Thread\"\n",
            "    ]\n",
            "  },\n",
            "  \"sendmsg_entry\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"1\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"Network\"\n",
            "    ]\n",
            "  },\n",
            "  \"close_fd\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"1\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"File\"\n",
            "    ]\n",
            "  },\n",
            "  \"process_freed\": {\n",
            "    \"properties\": {\n",
            "      \"weight\": {\n",
            "        \"type\": \"Integer\",\n",
            "        \"example_value\": \"1\"\n",
            "      }\n",
            "    },\n",
            "    \"startLabels\": [\n",
            "      \"Thread\"\n",
            "    ],\n",
            "    \"endLabels\": [\n",
            "      \"CPU\"\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Prepare a Schema Description for GPT**\n",
        "We’ll create a textual representation of the discovered schema so that GPT knows about labels, properties, and the communityId field."
      ],
      "metadata": {
        "id": "hEgBbz77ibdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Prepare a Schema Description for GPT\n",
        "\n",
        "def prepare_schema_description(node_schema, rel_schema):\n",
        "    desc = \"The knowledge graph has the following structure:\\n\\n\"\n",
        "    desc += \"Node labels and properties:\\n\"\n",
        "    for lab, props in node_schema.items():\n",
        "        desc += f\"- {lab}:\\n\"\n",
        "        for p, info in props.items():\n",
        "            desc += f\"  - {p} (type={info['type']}, example={info['example_value']})\\n\"\n",
        "    desc += \"\\nRelationship types and properties:\\n\"\n",
        "    for rt, info in rel_schema.items():\n",
        "        desc += f\"- {rt}:\\n\"\n",
        "        desc += f\"  - Connects from {info['startLabels']} to {info['endLabels']}\\n\"\n",
        "        desc += f\"  - Properties:\\n\"\n",
        "        for pk, pv in info['properties'].items():\n",
        "            desc += f\"    - {pk} (type={pv['type']}, example={pv['example_value']})\\n\"\n",
        "    return desc\n",
        "\n",
        "schema_description = prepare_schema_description(node_schema, rel_schema)\n",
        "print(schema_description)\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMP647JYierR",
        "outputId": "b4494920-e9e6-4b32-cb1c-53252d418b3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The knowledge graph has the following structure:\n",
            "\n",
            "Node labels and properties:\n",
            "- CPU:\n",
            "  - id (type=String, example=CPU_3)\n",
            "  - label (type=String, example=CPU 3)\n",
            "  - communityId (type=Integer, example=0)\n",
            "- Thread:\n",
            "  - id (type=String, example=T_0)\n",
            "  - label (type=String, example=swapper/3 (T_0))\n",
            "  - communityId (type=Integer, example=0)\n",
            "- File:\n",
            "  - id (type=String, example=File_140637496150960)\n",
            "  - label (type=String, example=Events FD=140637496150960)\n",
            "  - communityId (type=Integer, example=1)\n",
            "- Process:\n",
            "  - id (type=String, example=P_7)\n",
            "  - label (type=String, example=Process 7.0)\n",
            "  - communityId (type=Integer, example=0)\n",
            "- Network:\n",
            "  - id (type=String, example=Socket_FD_46)\n",
            "  - label (type=String, example=Socket FD 46)\n",
            "  - communityId (type=Integer, example=0)\n",
            "\n",
            "Relationship types and properties:\n",
            "- switched_in:\n",
            "  - Connects from ['CPU'] to ['Thread']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=44)\n",
            "- switched_out:\n",
            "  - Connects from ['Thread'] to ['CPU']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=44)\n",
            "- scheduled_to_wake_on:\n",
            "  - Connects from ['Thread'] to ['CPU']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=23)\n",
            "- wake_up:\n",
            "  - Connects from ['Thread'] to ['CPU']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=19)\n",
            "- epoll_wait_exit:\n",
            "  - Connects from ['Thread'] to ['File']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=1)\n",
            "- ioctl:\n",
            "  - Connects from ['Thread'] to ['File']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=393)\n",
            "- splice_in:\n",
            "  - Connects from ['Thread'] to ['File']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=98)\n",
            "- ioctl_exit:\n",
            "  - Connects from ['Thread'] to ['Thread']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=393)\n",
            "- splice_exit:\n",
            "  - Connects from ['Thread'] to ['Thread']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=196)\n",
            "- sync_file_range_exit:\n",
            "  - Connects from ['Thread'] to ['Thread']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=196)\n",
            "- splice_out:\n",
            "  - Connects from ['Thread'] to ['File']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=98)\n",
            "- sync_file:\n",
            "  - Connects from ['Thread'] to ['File']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=196)\n",
            "- runtime_stat:\n",
            "  - Connects from ['Thread'] to ['CPU']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=4)\n",
            "- contains:\n",
            "  - Connects from ['Process'] to ['Thread']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=1)\n",
            "- sendmsg_exit:\n",
            "  - Connects from ['Thread'] to ['Thread']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=1)\n",
            "- close_exit:\n",
            "  - Connects from ['Thread'] to ['Thread']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=1)\n",
            "- sendmsg_entry:\n",
            "  - Connects from ['Thread'] to ['Network']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=1)\n",
            "- close_fd:\n",
            "  - Connects from ['Thread'] to ['File']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=1)\n",
            "- process_freed:\n",
            "  - Connects from ['Thread'] to ['CPU']\n",
            "  - Properties:\n",
            "    - weight (type=Integer, example=1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: Generate Cypher Queries with GPT (Thorough Prompt Engineering)**\n",
        "Below is an extensive system prompt describing constraints. We also mention communityId so GPT can filter or group by communities if desired."
      ],
      "metadata": {
        "id": "7dMYJQ_PimKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    api_key=userdata.get(\"OPENAI_API_KEY\")\n",
        ")# Step 9: Generate Cypher Queries with GPT (Thorough Prompt Engineering)\n",
        "\n",
        "def generate_cypher_query(question, schema_description):\n",
        "    \"\"\"\n",
        "    Thorough system prompt for generating Cypher.\n",
        "    \"\"\"\n",
        "    system_prompt = f\"\"\"\n",
        "You are an expert in translating natural language questions into Cypher queries for a Neo4j graph database.\n",
        "\n",
        "Important guidelines:\n",
        "- Only use the provided schema information.\n",
        "- [VERY IMPORTANT]When generating the Cypher query, ensure that it returns all relevant nodes and relationships needed to answer the question.\n",
        "- Pay close attention to the data types, formats of node properties, and relationship directionality.\n",
        "- Node IDs and other properties may have specific formats (e.g., 'CPU_3' instead of '3').\n",
        "- Be aware of the direction of relationships and which node labels they connect.\n",
        "- When counting events, sum the 'weight' property of relationships instead of counting the number of relationships. The 'weight' property represents the number of occurrences or events.\n",
        "- When specifying multiple relationship types using the '|' operator in a Cypher query, include the colon ':' only once, before the first relationship type. Do NOT include colons before subsequent relationship types.\n",
        "- [THE MOST IMPORTANT]The semantics of using colon in the separation of alternative relationship types in conjunction with the use of variable binding, inlined property predicates, or variable length is no longer supported.\n",
        "  For example: r:switched_in|:switched_out|:scheduled_to_wake_on|: ... is WRONG. :switched_in|switched_out|scheduled_to_wake_on|... is correct.\n",
        "- Do not make up properties or labels that are not in the schema.\n",
        "- Generate a Cypher query that retrieves all relevant data needed to answer the question.\n",
        "- Include all relevant entities and relationships connected to the main entities.\n",
        "- Be mindful of potential token limits; if the result set is too large, you can limit the depth or the number of nodes appropriately.\n",
        "- Do not limit the number of results unless specified in the question.\n",
        "- Return the query without any explanations or additional text.\n",
        "- Always return a subgraph. It means you should write queries that return a subgraph including entities and relationships(For example a query that only return number of files is not ok and instead you should return)\n",
        "- If referencing community-based logic, use the node property 'communityId'.\n",
        "\n",
        "\n",
        "Schema:\n",
        "{schema_description}\n",
        "\"\"\"\n",
        "    user_prompt = f\"Question: {question}\\n\\nCypher Query:\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-4',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "            {\"role\": \"user\", \"content\": user_prompt.strip()}\n",
        "        ],\n",
        "        temperature=0.1\n",
        "    )\n",
        "    query = response.choices[0].message.content.strip()\n",
        "    return query\n"
      ],
      "metadata": {
        "id": "yXsck7DPiuly"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10 : Execute Cypher Queries**\n",
        "\"We define a simple function to run Cypher in Neo4j, returning the list of records."
      ],
      "metadata": {
        "id": "qexeeNpdi4cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Execute Cypher Queries\n",
        "\n",
        "def execute_cypher_query(query):\n",
        "    with driver.session() as session:\n",
        "        try:\n",
        "            result = session.run(query)\n",
        "            return [r.data() for r in result]\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "xRU7uHV5i70s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 11 : Summarize Results with GPT (Thorough Prompt Engineering)**\n",
        "We pass the raw query results (JSON) plus the user’s question to GPT for a final textual answer. We do token-limit checks to avoid errors."
      ],
      "metadata": {
        "id": "_uO6n3kai_j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Summarize Results with GPT\n",
        "\n",
        "def generate_final_answer(question, kg_data):\n",
        "    \"\"\"\n",
        "    Summarizes the knowledge graph data in answer to the question.\n",
        "    We preserve thorough prompt engineering from TAAF.\n",
        "    \"\"\"\n",
        "    if not kg_data.strip():\n",
        "        kg_data = \"No relevant data from the knowledge graph.\"\n",
        "\n",
        "    # Token counting to prevent over-limit\n",
        "    enc = tiktoken.encoding_for_model('gpt-4')\n",
        "    total_tokens = len(enc.encode(kg_data))\n",
        "    max_tokens = 7000  # approximate\n",
        "    if total_tokens > max_tokens:\n",
        "        # Truncate\n",
        "        ratio = max_tokens / total_tokens\n",
        "        trunc_len = int(len(kg_data) * ratio)\n",
        "        kg_data = kg_data[:trunc_len] + \"\\n[TRUNCATED DUE TO TOKEN LIMIT]\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a highly knowledgeable expert in trace analysis and knowledge graphs.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Knowledge Graph Data:\n",
        "{kg_data}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-4',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Use the provided KG data to provide a thorough and accurate answer.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "        ],\n",
        "        temperature=0.4\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "id": "VNeqnSpXjCMF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 12 : Local→Global (Community) Approach vs. Standard Approach**\n",
        "\"Inspired by the paper 'From Local to Global: A Graph RAG Approach to Query-Focused Summarization', we define a two-mode system:\n",
        "\n",
        "use_communities=False: single GPT-based query + summarization.\n",
        "use_communities=True: retrieve partial data per community, summarize each, then combine partial answers in a final step.\n",
        "This is an illustrative adaptation of the local→global pipeline in the paper.\""
      ],
      "metadata": {
        "id": "VvAwgog0jO0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Local->Global Approach vs. Standard Approach\n",
        "\n",
        "def get_all_community_ids():\n",
        "    \"\"\"\n",
        "    Return distinct communityIds from the loaded graph.\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "    MATCH (n)\n",
        "    WHERE n.communityId IS NOT NULL\n",
        "    RETURN DISTINCT n.communityId AS cid\n",
        "    ORDER BY cid\n",
        "    \"\"\"\n",
        "    records = execute_cypher_query(query)\n",
        "    return [r['cid'] for r in records] if records else []\n",
        "\n",
        "def get_subgraph_for_community(cid):\n",
        "    \"\"\"\n",
        "    Retrieve a subgraph (nodes and relationships) belonging to the given community.\n",
        "    This includes all nodes with communityId=cid and the relationships between them.\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "    MATCH (n)\n",
        "    WHERE n.communityId = {cid}\n",
        "    OPTIONAL MATCH (n)-[r]->(m)\n",
        "    WHERE m.communityId = {cid}\n",
        "    RETURN n, r, m\n",
        "    \"\"\"\n",
        "    return execute_cypher_query(query)\n",
        "\n",
        "def local_global_pipeline(question):\n",
        "    cids = get_all_community_ids()\n",
        "    partial_answers = []\n",
        "\n",
        "    for cid in cids:\n",
        "        # 1) Get subgraph\n",
        "        records = get_subgraph_for_community(cid)\n",
        "        if records:\n",
        "            sub_json = json.dumps(records, indent=2)\n",
        "\n",
        "            # 2) We remove any direct mention of \"Community X\" in the question\n",
        "            #    to reduce GPT referencing it in the partial answer\n",
        "            partial_answer = generate_final_answer(\n",
        "                question,  # <-- Use the original question\n",
        "                sub_json\n",
        "            )\n",
        "\n",
        "            # Instead of \"Community {cid} partial:\", keep it neutral\n",
        "            partial_answers.append(partial_answer)\n",
        "\n",
        "    # 3) Combine partial answers with minimal preamble\n",
        "    combined_text = \"\\n\".join(partial_answers)\n",
        "\n",
        "    final_prompt = f\"\"\"\n",
        "We retrieved partial answers to the question below from different subgraphs.\n",
        "\n",
        "**Question:**\n",
        "{question}\n",
        "\n",
        "**Partial Answers (from each subgraph):**\n",
        "{combined_text}\n",
        "\n",
        "Please provide a single, cohesive answer that integrates all of these partial findings without repeating them individually.\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-4',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Integrate partial subgraph answers into a cohesive overall response.\"},\n",
        "            {\"role\": \"user\", \"content\": final_prompt.strip()}\n",
        "        ],\n",
        "        temperature=0.4\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def standard_pipeline(question, schema_description):\n",
        "    \"\"\"\n",
        "    1) Generate a single Cypher query with GPT.\n",
        "    2) Execute it in Neo4j.\n",
        "    3) Summarize results with GPT.\n",
        "    \"\"\"\n",
        "    cypher_query = generate_cypher_query(question, schema_description)\n",
        "    print(\"Generated Cypher Query:\")\n",
        "    print(cypher_query)\n",
        "\n",
        "    records = execute_cypher_query(cypher_query)\n",
        "    if not records:\n",
        "        kg_data = \"No data found for your query.\"\n",
        "    else:\n",
        "        kg_data = json.dumps(records, indent=2)\n",
        "\n",
        "    final_answer = generate_final_answer(question, kg_data)\n",
        "    return final_answer\n"
      ],
      "metadata": {
        "id": "wJKcp1rZjR3P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 13 (Markdown): Main Answer Function**\n",
        "We define answer_question(question, use_communities) that toggles between the single-pass or local→global approach. This is our final user-facing function in TAAF."
      ],
      "metadata": {
        "id": "9RDpQ1zLjfGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Main Answer Function\n",
        "\n",
        "def answer_question(question, use_communities=False):\n",
        "    \"\"\"\n",
        "    If use_communities=False: standard single-pass TAAF approach.\n",
        "    If use_communities=True: local->global approach from the paper.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    if use_communities:\n",
        "        print(\"Running local->global pipeline (community-based).\")\n",
        "        answer = local_global_pipeline(question)\n",
        "    else:\n",
        "        print(\"Running standard single-pass pipeline.\")\n",
        "        answer = standard_pipeline(question, schema_description)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Answer generated in {end_time - start_time:.2f} seconds.\")\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "SczNij9vjhEL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 14 : Test the System**\n",
        "We can now test with a sample question. If use_communities=False, we do a single pass. If use_communities=True, we do a local→global approach referencing community-based partial answers."
      ],
      "metadata": {
        "id": "Rm7kv2M1jn28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Test the System\n",
        "\n",
        "sample_question_1 = \"What thread seems more important in the system and why?\"\n",
        "print(\"=== Standard Pipeline ===\")\n",
        "ans1 = answer_question(sample_question_1, use_communities=False)\n",
        "print(\"\\nAnswer (standard):\\n\", ans1)\n",
        "\n",
        "print(\"\\n\"+\"-\"*80+\"\\n\")\n",
        "\n",
        "sample_question_2 = \"What thread seems more important in the system and why?\"\n",
        "print(\"=== Local->Global Pipeline ===\")\n",
        "ans2 = answer_question(sample_question_2, use_communities=True)\n",
        "print(\"\\nAnswer (local->global):\\n\", ans2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI7PhY2Yjqr8",
        "outputId": "bbd3ad65-b2dc-4e81-e8cc-5369805d0f17"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Standard Pipeline ===\n",
            "Running standard single-pass pipeline.\n",
            "Generated Cypher Query:\n",
            "MATCH (t:Thread)-[r]->()\n",
            "RETURN t.id AS Thread_ID, sum(r.weight) AS Total_Weight\n",
            "ORDER BY Total_Weight DESC\n",
            "LIMIT 1\n",
            "Answer generated in 7.60 seconds.\n",
            "\n",
            "Answer (standard):\n",
            " Based on the provided Knowledge Graph data, the thread \"T_2208\" seems to be the most important in the system as it is the only thread data available. The \"Total_Weight\" of 1972 could indicate its importance, but without additional context or comparison to other threads, it's difficult to definitively assess its relative importance.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Local->Global Pipeline ===\n",
            "Running local->global pipeline (community-based).\n",
            "Answer generated in 88.70 seconds.\n",
            "\n",
            "Answer (local->global):\n",
            " Based on the information gathered from the knowledge graph data, several threads appear to be important in the system due to their various roles and interactions. \n",
            "\n",
            "The threads \"swapper/3 (T_0)\" and \"rcu_sched (T_7)\" are significant due to their frequent switching in and out across all CPUs, indicating their critical roles in managing the virtual memory system and scheduling tasks respectively. \n",
            "\n",
            "The thread \"lttng-consumerd (T_2208)\" is also of importance due to its frequent occurrence and involvement in a wide range of input/output operations and system calls. Similarly, the thread \"sh (T_5137)\" stands out due to its additional interactions with \"FD=0\", suggesting a more significant role in the system.\n",
            "\n",
            "The thread \"T_5135\" is noted for its involvement in more interactions within the system, while \"lttng (T_5123)\" is considered important due to its numerous interactions and association with \"Process 5123.0\" and \"FD 3\". \n",
            "\n",
            "The threads \"T_5143\" and \"T_5145\" seem to be equally important, performing similar operations and interactions, although \"T_5143\" appears more frequently in the data. The thread \"phoronix-test-s (T_5130)\" may also be of significance due to its interactions with different file descriptors.\n",
            "\n",
            "However, it's important to note that the importance of a thread can also depend on the specific context and tasks that the system is performing. For a more accurate assessment, additional data such as the CPU usage, memory usage, and the specific tasks that each thread is handling would be necessary.\n"
          ]
        }
      ]
    }
  ]
}