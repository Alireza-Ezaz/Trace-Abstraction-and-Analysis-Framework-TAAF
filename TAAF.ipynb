{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Install Necessary Packages**\n",
        "\n",
        "In this step, we ensure that all required Python packages are installed. These packages include:\n",
        "\n",
        "- **openai**: To interact with OpenAI's API for embeddings and language models.\n",
        "- **neo4j**: To connect and interact with the Neo4j graph database.\n",
        "- **tiktoken**: For token estimation, helping us manage token limits with OpenAI models.\n",
        "- **numpy**: For numerical computations, particularly vector operations."
      ],
      "metadata": {
        "id": "hv41dkiD8D2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install openai neo4j tiktoken numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h2Bh3mrj8liz",
        "outputId": "3dfca562-4118-459e-d112-fee4d6029b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.52.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-5.25.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2024.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Downloading openai-1.52.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading neo4j-5.25.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neo4j, jiter, h11, tiktoken, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 neo4j-5.25.0 openai-1.52.0 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Import Necessary Libraries**\n",
        "\n",
        "We import all the libraries that will be used throughout the notebook. This includes standard libraries and those we just installed."
      ],
      "metadata": {
        "id": "JAcuQCII8qA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # For handling JSON data\n",
        "import openai  # For OpenAI API interactions\n",
        "from neo4j import GraphDatabase  # For Neo4j database connection\n",
        "import tiktoken  # For token estimation with OpenAI models\n",
        "import numpy as np  # For numerical computations\n",
        "from google.colab import userdata  # For accessing user secrets in Colab"
      ],
      "metadata": {
        "id": "_JZl2hLg8umL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Set Up API Keys and Database Connections**\n",
        "\n",
        "Here, we set up the API key for OpenAI and establish a connection to the Neo4j database. We securely retrieve sensitive information using `userdata.get()`."
      ],
      "metadata": {
        "id": "A8pw4ugo84Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up OpenAI API key\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')  # Replace with your OpenAI API key\n",
        "\n",
        "# Set up Neo4j connection\n",
        "uri = userdata.get('NEO4J_URI')  # e.g., 'neo4j+s://xxxxxxxx.databases.neo4j.io'\n",
        "user = 'neo4j'\n",
        "password = userdata.get('NEO4J_PASSWORD')  # Replace with your Neo4j password\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))"
      ],
      "metadata": {
        "id": "x-E81qvE896R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Upload Knowledge Graph Data**\n",
        "\n",
        "We generate the knowledge graph from our trace dataset using `knowledge_graph_generator.py`, located in the `knowledge_graph` directory. This script outputs a graph in JSON format, which can be found in `output/knowledge_graph_output/knowledge_graph.json`. We then upload the `knowledge_graph.json` file, which contains the nodes and relationships of our knowledge graph. This data represents the entities and their connections within our system."
      ],
      "metadata": {
        "id": "GW9OH0L79IiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the knowledge graph data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the JSON file named 'knowledge_graph.json'\n",
        "with open('knowledge_graph.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract nodes and links\n",
        "nodes = data['nodes']\n",
        "links = data['links']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "G2wt3csW9fp6",
        "outputId": "3bf00af3-0c7a-4174-9c4d-62ffcd45bbed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c28f2334-2ad9-4ed4-b438-428ce062e681\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c28f2334-2ad9-4ed4-b438-428ce062e681\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving knowledge_graph.json to knowledge_graph.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Upload Event Translations Data**\n",
        "\n",
        "We generate human-readable translations of system events from the raw trace data using `trace_translator.py`, located in the `trace_translation` directory. This script outputs the translations in JSON format, which can be found in `output/trace_translation_output/event_translations.json`. We then upload the `event_translations.json` file, which contains these descriptions. This data will be used for semantic search and to provide context in our answers."
      ],
      "metadata": {
        "id": "NNjI3zsR-_ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the event translations JSON file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the JSON file named 'event_translations.json'\n",
        "with open('event_translations.json', 'r') as f:\n",
        "    event_translations = json.load(f)\n",
        "\n",
        "# Extract the traces (event descriptions)\n",
        "traces = event_translations['traces']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "-tKHQJwF_6Ad",
        "outputId": "7ec8a73d-ca14-498a-c8b0-52b811e0395e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-364c1564-64e7-45d9-b2b6-6b1e2ae65ff7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-364c1564-64e7-45d9-b2b6-6b1e2ae65ff7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving event_translations.json to event_translations.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Define Functions to Create Nodes and Relationships**\n",
        "\n",
        "We define helper functions to create nodes and relationships in the Neo4j database. These functions will be used to load our data into the neo4j graph database."
      ],
      "metadata": {
        "id": "_ag72gfcAbHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j.exceptions import ServiceUnavailable\n",
        "\n",
        "def create_nodes(tx, nodes):\n",
        "    \"\"\"Creates nodes in the Neo4j database.\"\"\"\n",
        "    for node in nodes:\n",
        "        query = f\"\"\"\n",
        "        MERGE (n:{node['entity']} {{\n",
        "            id: '{node['id']}',\n",
        "            label: '{node['label']}'\n",
        "        }})\n",
        "        \"\"\"\n",
        "        tx.run(query)\n",
        "\n",
        "def create_relationships(tx, links):\n",
        "    \"\"\"Creates relationships between nodes in the Neo4j database.\"\"\"\n",
        "    for link in links:\n",
        "        # Prepare properties, excluding certain keys\n",
        "        props = {k: v for k, v in link.items() if k not in ['source', 'target', 'relationship', 'key']}\n",
        "        prop_str = ', '.join([f\"{k}: '{v}'\" for k, v in props.items()])\n",
        "        prop_str = f\"{{{prop_str}}}\" if prop_str else ''\n",
        "        # Sanitize relationship name by replacing invalid characters with underscores\n",
        "        relationship_name = link['relationship'].replace(' ', '_').replace('=', '_')\n",
        "        query = f\"\"\"\n",
        "        MATCH (a {{id: '{link['source']}'}})\n",
        "        MATCH (b {{id: '{link['target']}'}})\n",
        "        MERGE (a)-[r:{relationship_name} {prop_str}]->(b)\n",
        "        \"\"\"\n",
        "        tx.run(query)"
      ],
      "metadata": {
        "id": "IUq4a5MfA13V"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Load Data into Neo4j Graph Database**\n",
        "\n",
        "Using the functions defined above, we load the nodes and relationships into the Neo4j database. This step populates the graph database with our knowledge graph."
      ],
      "metadata": {
        "id": "Nt3Wo7CYBPLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_into_neo4j(nodes, links):\n",
        "    \"\"\"Loads nodes and relationships into the Neo4j database.\"\"\"\n",
        "    with driver.session() as session:\n",
        "        try:\n",
        "            # Create nodes\n",
        "            session.write_transaction(create_nodes, nodes)\n",
        "            # Create relationships\n",
        "            session.write_transaction(create_relationships, links)\n",
        "            print(\"Data loaded successfully into Neo4j.\")\n",
        "        except ServiceUnavailable as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Execute the data loading\n",
        "load_data_into_neo4j(nodes, links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8bSmpbjWBXS3",
        "outputId": "b2b53d9f-e578-47d5-a796-0b1029d0b9d8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-8c66b0ac1a98>:6: DeprecationWarning: write_transaction has been renamed to execute_write\n",
            "  session.write_transaction(create_nodes, nodes)\n",
            "<ipython-input-32-8c66b0ac1a98>:8: DeprecationWarning: write_transaction has been renamed to execute_write\n",
            "  session.write_transaction(create_relationships, links)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully into Neo4j.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Retrieve Node Labels and Properties**\n",
        "\n",
        "We extract all node labels and their properties from the Neo4j database. This information is crucial for understanding the structure of our graph and for generating accurate Cypher queries in our future steps."
      ],
      "metadata": {
        "id": "WY6SwY95BgiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_node_labels_and_properties():\n",
        "    \"\"\"Retrieves node labels and their properties from the Neo4j database.\"\"\"\n",
        "    with driver.session() as session:\n",
        "        # Get all node labels\n",
        "        labels_result = session.run(\"CALL db.labels()\")\n",
        "        labels = [record['label'] for record in labels_result]\n",
        "\n",
        "        label_properties = {}\n",
        "        for label in labels:\n",
        "            properties_result = session.run(f\"\"\"\n",
        "            MATCH (n:`{label}`)\n",
        "            UNWIND keys(n) AS key\n",
        "            WITH key, head(collect(n[key])) AS value\n",
        "            RETURN DISTINCT key, value\n",
        "            \"\"\")\n",
        "            properties = {}\n",
        "            for record in properties_result:\n",
        "                key = record['key']\n",
        "                value = record['value']\n",
        "                # Determine data type\n",
        "                if isinstance(value, int):\n",
        "                    data_type = 'Integer'\n",
        "                elif isinstance(value, float):\n",
        "                    data_type = 'Float'\n",
        "                elif isinstance(value, bool):\n",
        "                    data_type = 'Boolean'\n",
        "                elif isinstance(value, list):\n",
        "                    data_type = 'List'\n",
        "                else:\n",
        "                    data_type = 'String'\n",
        "                properties[key] = {'type': data_type, 'example_value': str(value)}\n",
        "            label_properties[label] = properties\n",
        "        return label_properties"
      ],
      "metadata": {
        "id": "xLX9JZruBsub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Retrieve Relationship Types and Properties**\n",
        "\n",
        "We extract all relationship types and their properties, including which node labels they connect. This helps in understanding how entities are related in the graph."
      ],
      "metadata": {
        "id": "pGmmhkLHB5KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relationship_types_and_properties():\n",
        "    \"\"\"Retrieves relationship types and their properties from the Neo4j database.\"\"\"\n",
        "    with driver.session() as session:\n",
        "        # Get all relationship types\n",
        "        types_result = session.run(\"CALL db.relationshipTypes()\")\n",
        "        types = [record['relationshipType'] for record in types_result]\n",
        "\n",
        "        type_info = {}\n",
        "        for rel_type in types:\n",
        "            # Get the connected node labels and direction\n",
        "            connections_result = session.run(f\"\"\"\n",
        "            MATCH (start)-[r:`{rel_type}`]->(end)\n",
        "            RETURN DISTINCT labels(start) AS start_labels, labels(end) AS end_labels\n",
        "            \"\"\")\n",
        "            node_pairs = set()\n",
        "            for record in connections_result:\n",
        "                start_labels = record['start_labels']\n",
        "                end_labels = record['end_labels']\n",
        "                for start_label in start_labels:\n",
        "                    for end_label in end_labels:\n",
        "                        node_pairs.add((start_label, end_label))\n",
        "\n",
        "            # Get properties and example values\n",
        "            properties_result = session.run(f\"\"\"\n",
        "            MATCH ()-[r:`{rel_type}`]->()\n",
        "            UNWIND keys(r) AS key\n",
        "            WITH key, head(collect(r[key])) AS value\n",
        "            RETURN DISTINCT key, value\n",
        "            \"\"\")\n",
        "            properties = {}\n",
        "            for record in properties_result:\n",
        "                key = record['key']\n",
        "                value = record['value']\n",
        "                # Determine data type\n",
        "                if isinstance(value, int):\n",
        "                    data_type = 'Integer'\n",
        "                elif isinstance(value, float):\n",
        "                    data_type = 'Float'\n",
        "                elif isinstance(value, bool):\n",
        "                    data_type = 'Boolean'\n",
        "                elif isinstance(value, list):\n",
        "                    data_type = 'List'\n",
        "                else:\n",
        "                    data_type = 'String'\n",
        "                properties[key] = {'type': data_type, 'example_value': str(value)}\n",
        "\n",
        "            type_info[rel_type] = {\n",
        "                'properties': properties,\n",
        "                'start_labels': list(set([pair[0] for pair in node_pairs])),\n",
        "                'end_labels': list(set([pair[1] for pair in node_pairs]))\n",
        "            }\n",
        "        return type_info"
      ],
      "metadata": {
        "id": "Jenqs4wpCuCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10. Prepare Schema Description**\n",
        "\n",
        "We use the node and relationship information to create a textual schema description. This description will be provided to the language model to help it generate accurate Cypher queries."
      ],
      "metadata": {
        "id": "cI3bdKz6C31N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_schema_description():\n",
        "    \"\"\"Prepares a textual description of the database schema.\"\"\"\n",
        "    node_schema = get_node_labels_and_properties()\n",
        "    relationship_schema = get_relationship_types_and_properties()\n",
        "\n",
        "    schema_description = \"The knowledge graph has the following structure:\\n\\n\"\n",
        "    schema_description += \"Node labels, their properties, data types, and example values:\\n\"\n",
        "    for label, properties in node_schema.items():\n",
        "        schema_description += f\"- {label}:\\n\"\n",
        "        for prop, details in properties.items():\n",
        "            schema_description += f\"  - {prop} (type: {details['type']}, example: '{details['example_value']}')\\n\"\n",
        "\n",
        "    schema_description += \"\\nRelationship types, their properties, data types, example values, and connected node labels:\\n\"\n",
        "    for rel_type, info in relationship_schema.items():\n",
        "        schema_description += f\"- {rel_type}:\\n\"\n",
        "        schema_description += f\"  - Connects from {info['start_labels']} to {info['end_labels']}\\n\"\n",
        "        schema_description += \"  - Properties:\\n\"\n",
        "        for prop, details in info['properties'].items():\n",
        "            schema_description += f\"    - {prop} (type: {details['type']}, example: '{details['example_value']}')\\n\"\n",
        "        # Add note about weight property\n",
        "        if 'weight' in info['properties']:\n",
        "            schema_description += \"  - Note: The 'weight' property represents the number of occurrences or events for this relationship.\\n\"\n",
        "\n",
        "    return schema_description"
      ],
      "metadata": {
        "id": "VCqRCtjUDAZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11. Generate Embeddings for Event Translations (Translations RAG Step)**\n",
        "\n",
        "We generate vector embeddings for the event translations using OpenAI's embedding model. This is part of the Retrieval-Augmented Generation (RAG) process, where we retrieve relevant information from the event translations to enhance our answers.\n",
        "\n",
        "*Note:* This step might take several minutes"
      ],
      "metadata": {
        "id": "SYiJC73vDI2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=userdata.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "def get_embeddings(texts, model='text-embedding-ada-002'):\n",
        "    \"\"\"Generates embeddings for a list of texts using your OpenAI implementation.\"\"\"\n",
        "    embeddings = []\n",
        "    batch_size = 100  # Adjust based on rate limits\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        response = client.embeddings.create(input=batch, model=model)\n",
        "        batch_embeddings = [np.array(data.embedding) for data in response.data]\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Extract descriptions\n",
        "descriptions = [trace['translation'] for trace in traces]\n",
        "\n",
        "# Generate embeddings\n",
        "description_embeddings = get_embeddings(descriptions)"
      ],
      "metadata": {
        "id": "8ZmsggznDQBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12. Define Function to Retrieve Relevant Translations(Translations RAG Step)**\n",
        "\n",
        "We create a function to find the most relevant event translations based on a user's query by calculating the cosine similarity between embeddings."
      ],
      "metadata": {
        "id": "MFt9pVvBExlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_translations(query, descriptions, description_embeddings, top_k=20):\n",
        "    \"\"\"Retrieves relevant descriptions based on the user's query.\"\"\"\n",
        "    # Generate embedding for the query\n",
        "    response = client.embeddings.create(input=[query], model='text-embedding-ada-002')\n",
        "    query_embedding = np.array(response.data[0].embedding)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarities = np.dot(description_embeddings, query_embedding) / (\n",
        "        np.linalg.norm(description_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
        "    )\n",
        "\n",
        "    # Get top_k most similar descriptions\n",
        "    top_k_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    relevant_descriptions = [descriptions[i] for i in top_k_indices]\n",
        "    return relevant_descriptions"
      ],
      "metadata": {
        "id": "VQKGncNrEy4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13. Generate Cypher Queries from Natural Language Questions(KG RAG Step)**\n",
        "\n",
        "We define a function to translate natural language questions into Cypher queries using your OpenAI implementation, guided by the schema description."
      ],
      "metadata": {
        "id": "v1StJY21E-gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cypher_query(question, schema_description):\n",
        "    \"\"\"Generates a Cypher query based on the question and schema using your OpenAI implementation.\"\"\"\n",
        "    system_prompt = f\"\"\"\n",
        "You are an expert in translating natural language questions into Cypher queries for a Neo4j graph database.\n",
        "\n",
        "Important guidelines:\n",
        "- Only use the provided schema information.\n",
        "- [VERY IMPORTANT]When generating the Cypher query, ensure that it returns all relevant nodes and relationships needed to answer the question.\n",
        "- Pay close attention to the data types, formats of node properties, and relationship directionality.\n",
        "- Node IDs and other properties may have specific formats (e.g., 'CPU_3' instead of '3').\n",
        "- Be aware of the direction of relationships and which node labels they connect.\n",
        "- When counting events, sum the 'weight' property of relationships instead of counting the number of relationships. The 'weight' property represents the number of occurrences or events.\n",
        "- When specifying multiple relationship types using the '|' operator in a Cypher query, include the colon ':' only once, before the first relationship type. Do NOT include colons before subsequent relationship types.\n",
        "- Do not make up properties or labels that are not in the schema.\n",
        "- Generate a Cypher query that retrieves all relevant data needed to answer the question.\n",
        "- Include all relevant entities and relationships connected to the main entities.\n",
        "- Be mindful of potential token limits; if the result set is too large, you can limit the depth or the number of nodes appropriately.\n",
        "- Do not limit the number of results unless specified in the question.\n",
        "- Return the query without any explanations or additional text.\n",
        "\n",
        "Schema:\n",
        "{schema_description}\n",
        "\"\"\"\n",
        "    user_prompt = f\"\"\"\n",
        "Question: {question}\n",
        "\n",
        "Cypher Query:\n",
        "\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-4',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "            {\"role\": \"user\", \"content\": user_prompt.strip()}\n",
        "        ],\n",
        "        temperature=0.1\n",
        "    )\n",
        "    query = response.choices[0].message.content.strip()\n",
        "    return query"
      ],
      "metadata": {
        "id": "gto_XaVfFCZ0"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14. Execute Cypher Queries(KG RAG Step)**\n",
        "\n",
        "We define a function to execute the generated Cypher queries against the Neo4j database and retrieve the results."
      ],
      "metadata": {
        "id": "2GUlbbiOFOt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_cypher_query(query):\n",
        "    \"\"\"Executes the Cypher query and returns the results.\"\"\"\n",
        "    with driver.session() as session:\n",
        "        try:\n",
        "            result = session.run(query)\n",
        "            # Collect results\n",
        "            records = [record.data() for record in result]\n",
        "            return records\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "WBJNzBc1Fn0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **15. Generate the Final Answer**\n",
        "\n",
        "We generate the final answer, combining knowledge graph data and relevant event translations in addition to the user's query to provide a comprehensive response."
      ],
      "metadata": {
        "id": "Y5Zs390eG1jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_answer(question, kg_data, event_data):\n",
        "    \"\"\"Generates the final answer based on the question, knowledge graph data, and event data.\"\"\"\n",
        "    # Estimate the number of tokens in the context\n",
        "    encoding = tiktoken.encoding_for_model('gpt-4')\n",
        "    kg_tokens = len(encoding.encode(kg_data))\n",
        "    event_tokens = len(encoding.encode(event_data))\n",
        "    total_tokens = kg_tokens + event_tokens\n",
        "    max_context_tokens = 7000  # Adjust this based on the model's token limit\n",
        "\n",
        "    # If context is too large, truncate or summarize\n",
        "    if total_tokens > max_context_tokens:\n",
        "        # Truncate the longer of the two\n",
        "        if kg_tokens > event_tokens:\n",
        "            kg_data = kg_data[:int(len(kg_data) * (max_context_tokens / (2 * kg_tokens)))]\n",
        "            kg_data += \"\\n\\n[Data truncated due to token limit]\"\n",
        "        else:\n",
        "            event_data = event_data[:int(len(event_data) * (max_context_tokens / (2 * event_tokens)))]\n",
        "            event_data += \"\\n\\n[Data truncated due to token limit]\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a highly knowledgeable expert in trace analysis and knowledge graphs.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide detailed explanations and insights in your answers, utilizing both the data provided and your extensive expertise.\n",
        "your answer MUST use both kg data and event translations data in your answer (avoid using only one).\n",
        "\n",
        "Knowledge Graph relevant Data retrieved from database:\n",
        "{kg_data}\n",
        "\n",
        "Event Translations relevant data:\n",
        "{event_data}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-4',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You provide concise and accurate answers based on the data provided.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "        ],\n",
        "        temperature=0.4\n",
        "    )\n",
        "    answer = response.choices[0].message.content.strip()\n",
        "    return answer"
      ],
      "metadata": {
        "id": "HrH4AohjHBn9"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **16. Define the Main Answer Function**\n",
        "\n",
        "We tie everything together in a single function that takes a user's question and returns an answer by utilizing the functions defined above."
      ],
      "metadata": {
        "id": "kgzxREXCHTmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "    \"\"\"Answers the user's question using the knowledge graph and event translations.\"\"\"\n",
        "    # Prepare schema description\n",
        "    schema_description = prepare_schema_description()\n",
        "\n",
        "    # Generate Cypher query\n",
        "    query = generate_cypher_query(question, schema_description)\n",
        "    print(\"Generated Cypher Query:\")\n",
        "    print(query)\n",
        "\n",
        "    # Execute query\n",
        "    records = execute_cypher_query(query)\n",
        "    print(\"Query Results:\")\n",
        "    print(records)\n",
        "    if records is None or len(records) == 0:\n",
        "        kg_data = \"No data found for your query.\"\n",
        "    else:\n",
        "        # Serialize records to JSON\n",
        "        kg_data = json.dumps(records, indent=2)\n",
        "\n",
        "    # Retrieve relevant descriptions from event translations\n",
        "    relevant_translations = get_relevant_translations(question, descriptions, description_embeddings, top_k=5)\n",
        "    # Combine the descriptions into a single string\n",
        "    event_data = '\\n'.join(relevant_translations)\n",
        "\n",
        "    # Print the retrieved event translations\n",
        "    print(\"Retrieved Event Translations:\")\n",
        "    for idx, desc in enumerate(relevant_translations, 1):\n",
        "        print(f\"{idx}. {desc}\")\n",
        "\n",
        "    # Generate final answer\n",
        "    answer = generate_final_answer(question, kg_data, event_data)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "ewHMOaDNHWni"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **17. Test the System with a Sample Question**\n",
        "\n",
        "We test the entire system using a sample question to see how it performs and to verify that all components are working as expected."
      ],
      "metadata": {
        "id": "W5WhefF6Ho5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Are there any threads that depend on both CPU 3 and CPU 0?\"\n",
        "\n",
        "# Get the answer\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(\"\\nAnswer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFLT8ahMHxhZ",
        "outputId": "4a5c4d4b-b591-410f-a6a8-e6d31e5daffa"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':switched_in|switched_out|scheduled_to_wake_on|wake_up|runtime_stat|process_freed' instead)} {position: line: 2, column: 93, offset: 155} for query: \"MATCH (cpu0:CPU {label: 'CPU 0'}), (cpu3:CPU {label: 'CPU 3'})\\nMATCH path0=(cpu0)<-[:switched_in|:switched_out|:scheduled_to_wake_on|:wake_up|:runtime_stat|:process_freed]-(:Thread)\\nMATCH path3=(cpu3)<-[:switched_in|:switched_out|:scheduled_to_wake_on|:wake_up|:runtime_stat|:process_freed]-(:Thread)\\nWHERE ANY(node IN NODES(path0) WHERE node IN NODES(path3))\\nRETURN DISTINCT NODES(path0) AS ThreadsDependentOnBothCPU0AndCPU3\"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':switched_in|switched_out|scheduled_to_wake_on|wake_up|runtime_stat|process_freed' instead)} {position: line: 3, column: 93, offset: 274} for query: \"MATCH (cpu0:CPU {label: 'CPU 0'}), (cpu3:CPU {label: 'CPU 3'})\\nMATCH path0=(cpu0)<-[:switched_in|:switched_out|:scheduled_to_wake_on|:wake_up|:runtime_stat|:process_freed]-(:Thread)\\nMATCH path3=(cpu3)<-[:switched_in|:switched_out|:scheduled_to_wake_on|:wake_up|:runtime_stat|:process_freed]-(:Thread)\\nWHERE ANY(node IN NODES(path0) WHERE node IN NODES(path3))\\nRETURN DISTINCT NODES(path0) AS ThreadsDependentOnBothCPU0AndCPU3\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Cypher Query:\n",
            "MATCH (cpu0:CPU {label: 'CPU 0'}), (cpu3:CPU {label: 'CPU 3'})\n",
            "MATCH path0=(cpu0)<-[:switched_in|:switched_out|:scheduled_to_wake_on|:wake_up|:runtime_stat|:process_freed]-(:Thread)\n",
            "MATCH path3=(cpu3)<-[:switched_in|:switched_out|:scheduled_to_wake_on|:wake_up|:runtime_stat|:process_freed]-(:Thread)\n",
            "WHERE ANY(node IN NODES(path0) WHERE node IN NODES(path3))\n",
            "RETURN DISTINCT NODES(path0) AS ThreadsDependentOnBothCPU0AndCPU3\n",
            "Query Results:\n",
            "[{'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_0', 'label': 'swapper/3 (T_0)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_7', 'label': 'rcu_sched (T_7)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_2186', 'label': 'lttng-sessiond (T_2186)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_15322', 'label': 'kworker/0:0 (T_15322)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5134', 'label': 'sh (T_5134)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5136', 'label': 'sh (T_5136)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5137', 'label': 'sh (T_5137)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5139', 'label': 'sh (T_5139)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5140', 'label': 'php5 (T_5140)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5143', 'label': 'sh (T_5143)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5144', 'label': 'php5 (T_5144)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_2208', 'label': 'lttng-consumerd (T_2208)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_2559', 'label': 'kworker/3:0 (T_2559)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5123', 'label': 'lttng (T_5123)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5141', 'label': 'sh (T_5141)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5145', 'label': 'sh (T_5145)'}]}, {'ThreadsDependentOnBothCPU0AndCPU3': [{'id': 'CPU_0', 'label': 'CPU 0'}, {'id': 'T_5147', 'label': 'sh (T_5147)'}]}]\n",
            "Retrieved Event Translations:\n",
            "1. At 09:32:48.297 476 341, a task migration event occurred on CPU 3. The task from process 'sh', with TID 5142 and priority 20, was transferred from CPU 3 to CPU 0. This adjustment is typically performed to optimize process execution and load balancing across CPUs.\n",
            "2. At 09:32:48.288 451 323, on CPU 1, task sh (TID 5138, priority 20) woke up and targeted CPU 3.\n",
            "3. At 09:32:47.834 718 306, on CPU 3, task bench-all-event (TID 5119, priority 20) woke up and targeted CPU 0.\n",
            "4. At 09:32:48.279 576 998, a task migration event occurred on CPU 3. The task from process 'sh', with TID 5133 and priority 20, was transferred from CPU 3 to CPU 0. This adjustment is typically performed to optimize process execution and load balancing across CPUs.\n",
            "5. At 09:32:48.282 997 644, a task migration event occurred on CPU 3. The task from process 'sh', with TID 5135 and priority 20, was transferred from CPU 3 to CPU 0. This adjustment is typically performed to optimize process execution and load balancing across CPUs.\n",
            "\n",
            "Answer:\n",
            "Yes, there are several threads that depend on both CPU 3 and CPU 0. The threads identified from the knowledge graph data are: swapper/3 (T_0), rcu_sched (T_7), lttng-sessiond (T_2186), kworker/0:0 (T_15322), sh (T_5134, T_5136, T_5137, T_5139, T_5141, T_5143, T_5145, T_5147), php5 (T_5140, T_5144), lttng-consumerd (T_2208), kworker/3:0 (T_2559), and lttng (T_5123).\n",
            "\n",
            "The event translations data corroborates this, as it shows several task migration events from CPU 3 to CPU 0. For example, tasks from the 'sh' process with TIDs 5142, 5133, and 5135 were transferred from CPU 3 to CPU 0. This indicates that these threads are indeed dependent on both CPUs. The task migrations are typically performed to optimize process execution and load balancing across CPUs.\n"
          ]
        }
      ]
    }
  ]
}