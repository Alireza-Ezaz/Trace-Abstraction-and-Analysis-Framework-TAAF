# Trace Abstraction and Analysis Framework (TAAF)

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Directory Structure](#directory-structure)
- [Installation](#installation)
  - [Prerequisites](#prerequisites)
  - [Setup Steps](#setup-steps)
- [Configuration](#configuration)
- [Usage](#usage)
  - [Generating the Knowledge Graph](#generating-the-knowledge-graph)
  - [Translating Events](#translating-events)
  - [Running the Main Notebook](#running-the-main-notebook)
- [Dependencies](#dependencies)
- [Dataset](#dataset)
- [Output](#output)
- [Acknowledgements](#acknowledgements)

## Introduction

The **Trace Abstraction and Analysis Framework (TAAF)** is a comprehensive tool designed to abstract and analyze system traces using Large Language Models (LLMs) and Knowledge Graphs synergistically. This framework leverages the power of knowledge graphs to represent system events and LLMs to provide insightful translations and analyses, facilitating deeper understanding and debugging of complex systems.

## Features

- **Knowledge Graph Generation**: Converts raw trace data into structured knowledge graphs, capturing entities and their relationships.
- **Event Translation**: Translates low-level system events into human-readable descriptions using advanced translation scripts.
- **Visualization**: Provides intuitive visual representations of the knowledge graphs for easy analysis and interpretation.
- **Integration with Neo4j**: Utilizes Neo4j for efficient storage and querying of the knowledge graph.
- **Embeddings and Semantic Search**: Employs OpenAI's embedding models to enable semantic search over event translations.
- **Interactive Analysis**: Offers a Jupyter Notebook (`TAAF.ipynb`) for interactive querying and analysis of the knowledge graph.

## Directory Structure

```
taaf/
│
├── Knowledge_graph/
│   └── Knowledge_graph_generator.py
│
├── trace_translation/
│   ├── event_translator.py
│   └── translator_translator.py
│
├── trace_data/
│   ├── run0_0.csv
│   ├── run0_1.csv
│   ├── run0_2.csv
│   ├── run0_3.csv
│   ├── run0_4.csv
│   ├── run0_5.csv
│   ├── run0_6.csv
│   └── run0_7.csv
│
├── output/
│   ├── knowledge_graph_output/
│   │   ├── knowledge_graph.json
│   │   └── relationship_details.json
│   └── trace_translation_output/
│       ├── event_translations.txt
│       └── event_translations.json
│
├── TAAF.ipynb
│
├── requirements.txt
│
├── LICENSE
│
└── README.md
```

### Description of Directories and Files

- **Knowledge_graph/**: Contains scripts related to generating the knowledge graph from trace data.
  - `Knowledge_graph_generator.py`: Parses trace data and constructs the knowledge graph.

- **trace_translation/**: Contains scripts for translating system events.
  - `event_translator.py`: Translates raw events into human-readable descriptions.
  - `translator_translator.py`: Additional translation functionalities (details to be provided).

- **trace_data/**: Houses the dataset used for generating the knowledge graph.
  - `run0_0.csv` to `run0_7.csv`: CSV files containing trace data from ScimArk2.

- **output/**: Stores the results generated by the framework.
  - `knowledge_graph_output/`: Contains the generated knowledge graph and relationship details in JSON format.
    - `knowledge_graph.json`: The complete knowledge graph.
    - `relationship_details.json`: Detailed relationships extracted from the knowledge graph.
  - `trace_translation_output/`: Outputs from the event translation process.
    - `event_translations.txt`: Human-readable translations of events.
    - `event_translations.json`: JSON-formatted translations of events.

- **TAAF.ipynb**: The main Jupyter Notebook that orchestrates the entire framework, including data loading, processing, and interactive querying.

- **requirements.txt**: Lists all Python dependencies required to run the project.

- **LICENSE**: Contains the licensing information for the project.

- **README.md**: This documentation file.

## Installation

To set up and run TAAF, follow the steps below to configure your environment and install the necessary dependencies.

### Prerequisites

- **Python 3.7 or higher**: Ensure that Python is installed on your system. You can download it from [here](https://www.python.org/downloads/).
- **Git**: For cloning the repository. Download from [here](https://git-scm.com/downloads).
- **Jupyter Notebook**: To run the `TAAF.ipynb` notebook. Install via `pip` if not already installed.
- **Neo4j Database**: Set up a Neo4j instance. You can use [Neo4j Desktop](https://neo4j.com/download/) or [Neo4j Aura](https://neo4j.com/cloud/aura/) for a cloud-based solution.
- **OpenAI API Key**: Obtain an API key from [OpenAI](https://platform.openai.com/account/api-keys).

### Setup Steps

1. **Clone the Repository**

   ```bash
   git clone https://github.com/yourusername/taaf.git
   cd taaf
   ```

2. **Create a Virtual Environment (Optional but Recommended)**

   ```bash
   python3 -m venv taaf_env
   source taaf_env/bin/activate  # On Windows: taaf_env\Scripts\activate
   ```

3. **Install Dependencies**

   Install the required Python packages using `pip`:

   ```bash
   pip install -r requirements.txt
   ```

   *If `requirements.txt` is not provided, you can install dependencies manually:*

   ```bash
   pip install pandas networkx matplotlib jupyter openai neo4j tiktoken numpy
   ```

4. **Configure Environment Variables**

   Create a `.env` file in the root directory and add your OpenAI and Neo4j credentials:

   ```env
   OPENAI_API_KEY=your_openai_api_key
   NEO4J_URI=your_neo4j_uri  # e.g., neo4j+s://xxxxxxxx.databases.neo4j.io
   NEO4J_PASSWORD=your_neo4j_password
   ```

   *Alternatively, you can set these variables in your environment directly.*

## Configuration

Before running the framework, ensure that you have:

- **OpenAI API Key**: Set in the environment variable `OPENAI_API_KEY`.
- **Neo4j Database**: Accessible via the URI specified in `NEO4J_URI` with the corresponding password in `NEO4J_PASSWORD`.
- **Trace Data**: Ensure that the `trace_data/` directory contains all necessary CSV files (`run0_0.csv` to `run0_7.csv`).

## Usage

### Generating the Knowledge Graph

1. **Navigate to the Knowledge Graph Directory**

   ```bash
   cd Knowledge_graph
   ```

2. **Run the Knowledge Graph Generator Script**

   ```bash
   python Knowledge_graph_generator.py
   ```

   This script will process the trace data and generate a knowledge graph, saving the outputs in the `output/knowledge_graph_output/` directory.

### Translating Events

1. **Navigate to the Trace Translation Directory**

   ```bash
   cd ../trace_translation
   ```

2. **Run the Event Translator Scripts**

   ```bash
   python event_translator.py
   python translator_translator.py
   ```

   These scripts will translate raw events into human-readable descriptions, saving the outputs in the `output/trace_translation_output/` directory.

### Running the Main Notebook

1. **Navigate Back to the Root Directory**

   ```bash
   cd ..
   ```

2. **Launch Jupyter Notebook**

   ```bash
   jupyter notebook
   ```

3. **Open `TAAF.ipynb`**

   In the Jupyter interface, navigate to `TAAF.ipynb` and open it.

4. **Follow the Notebook Steps**

   The notebook will guide you through:

   - Installing necessary packages.
   - Importing libraries.
   - Setting up API keys and database connections.
   - Uploading and loading knowledge graph and event translation data.
   - Loading data into the Neo4j graph database.
   - Generating embeddings for event translations.
   - Defining functions for semantic search and query generation.
   - Answering sample questions based on the knowledge graph and event translations.

## Dependencies

The project relies on the following Python libraries:

- **pandas**: Data manipulation and analysis.
- **networkx**: Creation, manipulation, and study of complex networks.
- **matplotlib**: Plotting and visualization.
- **json**: Handling JSON data.
- **openai**: Interacting with OpenAI's API for embeddings and language models.
- **neo4j**: Connecting and interacting with the Neo4j graph database.
- **tiktoken**: Token estimation for managing token limits with OpenAI models.
- **numpy**: Numerical computations.
- **Jupyter Notebook**: Interactive computing environment.

These dependencies are listed in the `requirements.txt` file for easy installation.

## Dataset

The framework utilizes trace data from **ScimArk2**, stored in the `trace_data/` directory as CSV files (`run0_0.csv` to `run0_7.csv`). Ensure that all dataset files are present before running the framework. These files contain detailed system trace information used to generate the knowledge graph and translate events.

## Output

The results generated by TAAF are stored in the `output/` directory, organized as follows:

- **knowledge_graph_output/**:
  - `knowledge_graph.json`: The structured knowledge graph representing the system trace.
  - `relationship_details.json`: Detailed information about relationships within the knowledge graph.

- **trace_translation_output/**:
  - `event_translations.txt`: Human-readable translations of events.
  - `event_translations.json`: JSON-formatted translations of events.

These outputs can be used for visualization, analysis, and further research.

## Acknowledgements

- **ScimArk2**: For providing the dataset used in this framework.
- **OpenAI**: For developing the language models that inspired the integration with LLMs.
- **Neo4j**: For offering a powerful graph database solution.
- **Community Contributors**: Special thanks to all contributors who have supported and improved TAAF.

---

*Feel free to reach out via [issues](https://github.com/yourusername/taaf/issues) for any queries or support.*
